{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Spark\n",
    "Using Spark we are going to read in this data and calculate the average age. First, we need to initialize a SparkSession:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# Set spark environments\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Spark Example\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s go ahead and create a Spark Dataset from our Lord of the Rings age data. Included in the Spark directory for this chapter is a file called ages.json which includes the age data in JSON lines format. It looks like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "{\"Name\": \"Bilbo\", \"Age\": 28}\n",
    "{\"Name\": \"Frodo\", \"Age\": 26}\n",
    "{\"Name\": \"Gandalf\", \"Age\": 62}\n",
    "{\"Name\": \"Samwise\", \"Age\": 30}\n",
    "{\"Name\": \"Sauron\", \"Age\": 72}\n",
    "{\"Name\": \"Aragorn\", \"Age\": 31}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can read in `ages.json` as a Spark Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json('ages.json').repartition(10).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a Dataset (also called DataFrame in accordance with Pandas) representing our data. We can leverage the Spark SQL API to calculate an aggregation over the dataset, which in our case is an average:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(avg(Age)=41.5)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.agg({\"Age\": \"avg\"}).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also execute calculations at the row level. For example, let’s calculate each of the character’s age in dog years (age times 7):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Age=72, Name='Sauron', dog_years=504),\n",
       " Row(Age=31, Name='Aragorn', dog_years=217),\n",
       " Row(Age=62, Name='Gandalf', dog_years=434),\n",
       " Row(Age=28, Name='Bilbo', dog_years=196),\n",
       " Row(Age=26, Name='Frodo', dog_years=182),\n",
       " Row(Age=30, Name='Samwise', dog_years=210)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.withColumn('dog_years', df.Age*7).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best of all, this calculation would have scaled automatically across our computing cluster if we had more than one node. Notice something at the end of each of the commands above? If you are thinking, what does `.collect()` do then you’re onto something. \n",
    "\n",
    "Spark executes code lazily. This means that *transformations* such as calculating the characters’ age in dog years is only executed once an *action* is called. The `.withColumn()` command is a *transformation* while `.collect()` is the *action* which causes the *transformation* to be executed. Often, the *action* which causes execution of our *transformations* is writing the job’s output to disk, HDFS, or S3.\n",
    "\n",
    "Let’s try to create a new Dataset which includes the characters’ ages in dog years, then let’s write this out to disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df.withColumn('dog_years', df.Age*7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a new Dataset called `df_new`. Note that nothing has been calculated yet; we have simply mapped the function we want across the cluster so that when we call an action on `df_new` such as `.collect()` or try to write the output to disk the transformation will be executed.\n",
    "\n",
    "We can write `df_new` to disk with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.write.mode('append').json(\"dog_years.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even execute a filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = df.filter(\"name = 'Bilbo'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Age=28, Name='Bilbo')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
